# ğŸ§  Edge-Cloud AI Arbitrage Kernel

**A Hybrid Compute Governance Engine for Cost-Aware, Privacy-Aware LLM Orchestration**

[![Python 3.14+](https://img.shields.io/badge/python-3.14%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Built with Ollama](https://img.shields.io/badge/Edge-Ollama-orange.svg)](https://ollama.com/)
[![Cloud: Gemini](https://img.shields.io/badge/Cloud-Gemini%20API-4285F4.svg)](https://ai.google.dev/)

---

## What Is This?

A Python-based runtime that **dynamically allocates AI inference workloads between local edge compute (Ollama on Apple Silicon) and cloud APIs (Google Gemini)**, making routing decisions based on explicit policies for **cost, latency, privacy, and task complexity**.

Every routing decision is logged, auditable, and explainable. Every dollar of cloud spend is tracked against a full-cloud baseline to measure **cost avoidance**. Sensitive data never leaves the device.

> **This is not a chatbot. It is a governance layer for AI compute allocation.**

---

## Why Does This Exist?

Most AI applications send every request to the cloud by default. This is:

- **Expensive** â€” cloud API costs scale linearly with usage
- **Slow** â€” network round-trips add latency
- **Risky** â€” sensitive data leaves your control

The Arbitrage Kernel asks a simple question before every inference call:

> *"Does this task NEED the cloud, or can local hardware handle it?"*

By routing simple, low-stakes, or privacy-sensitive tasks to local models and reserving cloud compute for complex, high-importance tasks, the system achieves:

- **60â€“70% of workloads processed locally at zero marginal cost**
- **Enterprise-grade privacy enforcement** â€” HIGH sensitivity data never leaves the device
- **Budget governance** â€” soft and hard spending caps with automatic downgrade policies
- **Full observability** â€” every call logged with model, tier, latency, tokens, cost, and routing reason

---

## Key Features

| Feature | Description |
|---|---|
| ğŸ”€ **Explicit Routing Engine** | Rule-based model selection using task type, complexity, importance, privacy level, and budget state. Every decision includes a human-readable reason. |
| ğŸ”’ **Privacy Gating** | `HIGH` sensitivity tasks are forced to edge compute. No exceptions. No overrides. A non-negotiable policy enforced at the routing layer. |
| ğŸ’° **Budget Cap Enforcement** | Soft budget triggers model downgrade warnings. Hard budget blocks all cloud calls. Month-to-date spend tracked automatically. |
| ğŸ“Š **Full Observability & Logging** | Every inference call logged to SQLite: model, tier, task type, latency, tokens, actual cost, and hypothetical cloud cost. |
| ğŸ¤– **Multi-Agent Research Council** | Three-agent pipeline (Explainer â†’ Skeptic â†’ Synthesizer) that produces structured JSON output with assumptions, risks, disagreements, and confidence scores. |
| âœ… **Contract Tests** | Pytest-based schema validation ensures council output is always parseable and compliant â€” production discipline, not prototype behavior. |
| ğŸ“ˆ **Benchmarking Framework** | 8 task scenarios across 6 models (2 edge + 4 cloud), producing CSV data and auto-generated Markdown comparison reports. |
| ğŸ–¥ï¸ **Executive Dashboard** | Streamlit dashboard showing cost avoidance, edge/cloud distribution, latency comparison, cumulative spend, and governance events. |

---

Architecture
------------

```text
User Task
    â†“
TaskMetadata (type, complexity, sensitivity, importance)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ROUTING ENGINE           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Privacy   â”‚      â”‚   Budget     â”‚â”‚
â”‚  â”‚  Policy   â”‚      â”‚   Policy     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚          Task-Based Routing          â”‚
â”‚        (complexity Ã— importance)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    EDGE    â”‚   â”‚       CLOUD        â”‚
      â”‚   Ollama   â”‚   â”‚     Gemini API     â”‚
      â”‚ Llama 3.1  â”‚   â”‚ 2.5 Flash / Pro    â”‚
      â”‚ DeepSeek R1â”‚   â”‚ 3.0 Pro Preview    â”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                   â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   LOGGING LAYER   â”‚
      â”‚ SQLite + Cost     â”‚
      â”‚ Tracking          â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   ROI DASHBOARD   â”‚
      â”‚ Cost Avoidance    â”‚
      â”‚ Edge/Cloud %      â”‚
      â”‚ Governance Events â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



### Routing Decision Hierarchy

Decisions are evaluated **in strict order** â€” the first matching rule wins:

1. **Forced Override** â†’ Use the explicitly specified model
2. **Privacy Check** â†’ `HIGH` sensitivity â†’ forced to edge (non-negotiable)
3. **Privacy Mode** â†’ `edge_only` mode â†’ everything local
4. **Hard Budget** â†’ Monthly cloud spend â‰¥ hard cap â†’ forced to edge
5. **Soft Budget** â†’ Monthly cloud spend â‰¥ soft cap â†’ downgrade or prefer edge
6. **Task Routing** â†’ Match task type + complexity + importance to model capabilities
7. **Default** â†’ Edge (minimize cost)

---

## Model Inventory

### Edge Models (Local â€” Ollama on Apple Silicon)

| Model | ID | Parameters | Quantization | Strengths |
|---|---|---|---|---|
| **Llama 3.1 8B Instruct** | `ollama/llama3.1-8b` | 8B | Q5_K_M | Strong general-purpose instruction following, balanced quality/speed |
| **DeepSeek R1 8B** | `ollama/deepseek-r1-8b` | 8B | Default | Excellent reasoning and chain-of-thought, strong at code and analysis |

### Cloud Models (Google Gemini API)

| Model | ID | Tier | Best For |
|---|---|---|---|
| **Gemini 2.5 Flash** | `gemini/gemini-2.5-flash` | Standard | Fast general-purpose tasks, good cost/quality balance |
| **Gemini 2.5 Flash Lite** | `gemini/gemini-2.5-flash-lite` | Economy | High-volume, cost-sensitive tasks |
| **Gemini 2.5 Pro** | `gemini/gemini-2.5-pro` | Premium | Complex reasoning, deep research, high-stakes analysis |
| **Gemini 3.0 Pro Preview** | `gemini/gemini-3-pro-preview` | Cutting-edge | Latest capabilities, frontier model performance |

---

## Quick Start

### Prerequisites

- **macOS with Apple Silicon** (M1/M2/M3/M4 â€” developed on M1 Pro 16GB)
- **Python 3.14+**
- **Ollama** installed and running
- **Google Gemini API key** (free tier)

### 1. Clone the Repository

```bash
git clone https://github.com/im-AbhiP/edge-cloud-arbitrage-kernel.git
cd edge-cloud-arbitrage-kernel
```

### 2. Set Up the Python Environment

```bash
python3 -m venv .venv
source .venv/bin/activate

pip install httpx python-dotenv pyyaml rich
pip install pytest ruff
pip install streamlit plotly pandas
```

### 3. Install and Start Ollama

```bash
brew install ollama

ollama serve
ollama pull llama3.1:8b-instruct-q5_K_M
ollama pull deepseek-r1:8b
```

### 4. Configure Environment Variables
Create a .env file in the project root:

```text
GEMINI_API_KEY=your_gemini_api_key_here
PRIVACY_MODE=hybrid
SOFT_BUDGET_USD=1.00
HARD_BUDGET_USD=5.00
```

Get your free Gemini API key at Google AI Studio.

### 5. Run the Smoke Test

```bash
python test_smoke.py
```

You should see three tests execute:
Test 1: Simple Q&A â†’ routed to edge (Llama 3.1 8B)
Test 2: Deep research â†’ routed to cloud (Gemini 2.5 Pro)
Test 3: Sensitive data â†’ forced to edge regardless of complexity
### Basic: Route a Single Task

```python
from runtime.tasks import TaskMetadata, TaskType, DataSensitivity
from runtime.router import ModelRouter

router = ModelRouter()

# Simple question â†’ routes to local model (free)
result = router.run(
    prompt="What is the difference between TCP and UDP?",
    meta=TaskMetadata(
        task_type=TaskType.QUICK_QA,
        complexity=0.2,
        importance=0.3,
        budget_sensitivity=0.8,
    ),
)

print(result.text)
print(f"Model: {result.model_name} | Cost: $0.00")
```

### Privacy-Enforced Task

```python
# Sensitive data â†’ ALWAYS stays local, no matter what
result = router.run(
    prompt="Analyze this employee's compensation data...",
    meta=TaskMetadata(
        task_type=TaskType.DATA_ANALYSIS,
        complexity=0.9,
        importance=0.9,
        data_sensitivity=DataSensitivity.HIGH,  # Forces edge
    )
)
# Guaranteed: result.tier == "edge"
```

### Multi-Agent Research Council

```python
from council.agents import ResearchCouncil
council = ResearchCouncil() output = council.run( "What are the tradeoffs of edge vs cloud AI inference?" )
print(output["council_output"]["summary"]) print(output["council_output"]["risks"]) print(output["council_output"]["confidence"])
```

Output is structured JSON with assumptions, risks, disagreements.

### Run Benchmarks

```bash
python -m benchmarking.run_benchmarks
```

Runs 8 task scenarios across all 6 available models and generates:
 - `benchmarking/reports/benchmark_YYYYMMDD_HHMMSS.csv`
 - `benchmarking/reports/benchmark_YYYYMMDD_HHMMSS.md`

### Generate ROI Report

```bash
python scripts/summarize_logs.py
```

### Launch the Dashboard

```bash
streamlit run dashboard/streamlit_app.py
```

Opens an interactive dashboard at http://localhost:8501.
## Project Structure

```text
edge-cloud-arbitrage-kernel/
â”œâ”€â”€ runtime/                   # Core inference engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # Model clients (Ollama, Gemini)
â”‚   â”œâ”€â”€ tasks.py               # TaskMetadata, TaskType, DataSensitivity
â”‚   â”œâ”€â”€ router.py              # Routing engine with policy enforcement
â”‚   â”œâ”€â”€ prompts.py             # Prompt registry loader
â”‚   â”œâ”€â”€ prompts.yaml           # Prompt templates with expected tokens
â”‚   â”œâ”€â”€ logging_utils.py       # SQLite logging, cost estimation, ROI stats
â”‚   â””â”€â”€ policies.py            # Policy configuration
â”‚
â”œâ”€â”€ council/                   # Multi-agent research pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents.py              # Explainer â†’ Skeptic â†’ Synthesizer
â”‚   â”œâ”€â”€ schemas.py             # JSON schema for structured output
â”‚   â””â”€â”€ contract_tests.py      # Pytest schema validation
â”‚
â”œâ”€â”€ benchmarking/              # Empirical evaluation framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run_benchmarks.py      # Benchmark runner (8 tasks Ã— 6 models)
â”‚   â”œâ”€â”€ benchmark_dataset.yaml # Benchmark scenarios
â”‚   â””â”€â”€ reports/               # Auto-generated CSV + Markdown reports
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ streamlit_app.py       # Executive ROI dashboard
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ summarize_logs.py      # CLI ROI report generator
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ analyze_logs.py        # Ad-hoc log analysis
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ logs.db                # SQLite call log database (auto-created)
â”‚
â”œâ”€â”€ reports/                   # Generated ROI summaries
â”œâ”€â”€ test_smoke.py              # End-to-end smoke test
â”œâ”€â”€ .env                       # API keys & config (not committed)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## How Routing Decisions Work

Every routing decision is transparent and auditable. The router logs not just which model was selected, but **why**.

### Routing Examples
| Scenario                               | Decision                     | Reason                                       |
|----------------------------------------|------------------------------|----------------------------------------------|
| Quick Q&A, low importance              | ollama/llama3.1-8b           | Simple task + cost-conscious â†’ fast local    |
| Deep research, high complexity         | gemini/gemini-2.5-pro        | Complex + important â†’ premium cloud model    |
| Reasoning-heavy, moderate importance   | ollama/deepseek-r1-8b        | Reasoning task â†’ DeepSeek R1 excels locally  |
| High-volume, low-stakes task           | gemini/gemini-2.5-flash-lite | Economy tier â†’ minimize cloud cost           |
| Any task with HIGH data sensitivity    | ollama/llama3.1-8b           | HIGH sensitivity â€” forced to edge            |
| Any task when budget exceeded          | ollama/llama3.1-8b           | Hard budget exceeded â†’ edge only             |
| Cloud API failure                      | ollama/llama3.1-8b           | Fallback after cloud failure                 |
| Frontier-capability needed             | gemini/gemini-3-pro-preview  | Cutting-edge task â†’ latest model             |


## Cost Model
| Model                        | Tier             | Cost               |
|-----------------------------|------------------|--------------------|
| ollama/llama3.1-8b          | Edge             | $0.00              |
| ollama/deepseek-r1-8b       | Edge             | $0.00              |
| gemini/gemini-2.5-flash-lite| Cloud (Economy)  | Per-token pricing  |
| gemini/gemini-2.5-flash     | Cloud (Standard) | Per-token pricing  |
| gemini/gemini-2.5-pro       | Cloud (Premium)  | Per-token pricing  |
| gemini/gemini-3-pro-preview | Cloud (Frontier) | Per-token pricing  |

For every edge call, the system calculates the hypothetical cloud cost â€” what it would have cost if sent to Gemini 2.5 Flash. The difference is the cost avoidance metric that powers the ROI dashboard.
### Sample ROI Output

```text
Edge-Cloud AI Arbitrage Kernel â€” ROI Report

Executive Summary
-----------------
Total AI calls: 46
Edge (local) calls: 31 (67.4%)
Cloud calls: 15 (32.6%)

Total cloud cost: $0.0123
Cost if ALL calls were cloud: $0.0970
ğŸ’° Cost avoided: $0.0847

Average latency: 1,847ms
Success rate: 97.8%
```
---

## Testing

### Contract Tests

```bash
pytest council/contract_tests.py -v
```

Validates:
 - âœ… All required keys present (summary, assumptions, risks, disagreements, confidence)
 - âœ… Confidence score is a number in [0.0, 1.0]
 - âœ… assumptions and risks are lists 
 - âœ… Summary is a non-empty string

###Smoke Test

```bash
python test_smoke.py
```

Validates the full pipeline: task metadata â†’ routing â†’ model call â†’ logging â†’ stats.

## Tech Stack
| Component        | Technology                 | Why                                                   |
|------------------|----------------------------|-------------------------------------------------------|
| Language         | Python 3.14                | Latest stable release, performance improvements       |
| Local Inference  | Ollama                     | Best local LLM runtime for Apple Silicon, REST API    |
| Edge Models      | Llama 3.1 8B, DeepSeek R1  | Instruction-following + reasoning coverage            |
| Cloud Inference  | Google Gemini API          | Generous free tier, strong quality, simple REST API   |
| HTTP Client      | httpx                      | Modern, async-capable, no SDK dependency              |
| Database         | SQLite                     | Zero-config, built into Python, SQL-queryable         |
| Config           | YAML + .env                | Human-readable config, secure credential management   |
| Testing          | pytest                     | Industry standard, clean syntax                       |
| Dashboard        | Streamlit + Plotly         | Rapid interactive dashboards with minimal code        |
| Linter           | Ruff                       | Faster than flake8, replaces black + isort            |
| IDE              | PyCharm Professional       | Best-in-class Python IDE with database tools          |

## Dashboard:



The Streamlit dashboard provides five key views:
 - ğŸ’° Cost Avoidance â€” Dollars saved vs. a full-cloud baseline 
 - ğŸ  Edge/Cloud Distribution â€” Pie chart of compute allocation 
 - âš¡ Latency Comparison â€” Bar chart of average latency per model 
 - ğŸ“ˆ Cumulative Cost â€” Line chart comparing hybrid cost vs. cloud-only cost over time 
 - ğŸ”’ Governance Events â€” Count of privacy and budget enforcement actions

Launch with: 
`streamlit run dashboard/streamlit_app.py`
![Dashboard Screenshot](./reports/dashboard_screenshot.png)

## Roadmap
âœ… v1 (Current)
 - Model client abstraction (Ollama + Gemini, 6 models)
 - Explicit routing engine with privacy & budget policies 
 - SQLite logging with cost tracking and ROI computation 
 - Prompt registry with expected token counts 
 - Benchmarking framework (8 scenarios Ã— 6 models)
 - Multi-agent research council with structured JSON output 
 - Contract tests for output schema validation 
 - Executive Streamlit dashboard 
 - CLI ROI report generator

ğŸ”œ v2 (Planned â€” Choose One)
 - Performance-Adaptive Router â€” Use aggregate log statistics to auto-adjust routing thresholds 
 - Task Decomposition â€” Split complex tasks: cheap local chunking â†’ expensive cloud synthesis 
 - Confidence-Based Escalation â€” If council confidence < threshold, re-route to a stronger model

ğŸ”® Future Directions
 - OpenRouter integration for access to 100+ models through a single API 
 - Adaptive routing weights learned from historical performance data 
 - Context budget optimization for long-document tasks 
 - Multi-tenant support with per-user privacy and budget policies 
 - MassGen integration for advanced agent orchestration

## Design Principles
 + Explicit over Magic â€” Routing rules are readable if-else logic, not opaque ML models. Every decision is explainable in plain English. 
 + Measure Everything â€” If it's not logged, it didn't happen. Every call captures model, tier, latency, tokens, cost, and routing reason. 
 + Privacy is Non-Negotiable â€” HIGH sensitivity data never leaves the device. Enforced at the routing layer, not left to the caller. 
 + Ship, Then Improve â€” v1 uses simple rules. v2 adds data-driven optimization. Complexity is earned, not assumed. 
 + Interview-Ready at Every Commit â€” Code, tests, documentation, and dashboards are always in a presentable state.

## The 60-Second Explanation
> I built a hybrid AI orchestration kernel that dynamically allocates inference workloads between edge and cloud based on cost, complexity, and privacy constraints.
> The edge tier runs Llama 3.1 and DeepSeek R1 locally on Apple Silicon at zero marginal cost, while the cloud tier leverages four Gemini models from economy to frontier.
> Every routing decision is explainable and auditable. Sensitive data never leaves the device â€” that's a non-negotiable policy enforced at the routing layer.
> In testing, the system routed approximately 60â€“70% of workloads to local inference while maintaining comparable output quality.
> I validated this with empirical benchmarks across 8 task types and 6 models, and built an executive dashboard that visualizes cost avoidance, compute distribution, and governance events in real time.


### License
This project is licensed under the MIT License â€” see the LICENSE file for details.

### Author
Abhishek\
Senior Technical Product Manager @ AMD | Georgia Tech

Building at the intersection of AI infrastructure, edge compute, and hybrid cloud governance.\
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://linkedin.com/in/abhishekhpatil)\
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black)](https://github.com/im-AbhiP)
