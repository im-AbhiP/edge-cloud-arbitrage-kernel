"""
System prompts for the Council of Local LLMs.

WHAT THIS FILE DOES:
This file contains all the system prompts (instructions) that turn
your 5 local models into council members. Each deliberation phase
has its own prompt function that generates the exact text sent to
each model.

WHY A SEPARATE PROMPTS FILE?
Separating prompts from logic is a common pattern in LLM engineering.
It means:
1. You can iterate on prompts without touching Python logic.
2. Prompts are version-controlled and easy to review.
3. A non-programmer (e.g., a PM) could improve the prompts
   without understanding the deliberation engine code.

THREE PHASES, THREE PROMPT FUNCTIONS:
1. get_routing_vote_prompt()    - "Should this go to cloud or stay local?"
2. get_model_selection_prompt() - "Which local model should answer?"
3. get_answer_review_prompt()   - "Is this answer good enough?"

Plus two helper functions:
- format_model_profiles()  - Turns model profile objects into readable text
- format_task_metadata()   - Turns TaskMetadata into readable text

DESIGN PRINCIPLE:
Each prompt tells the model EXACTLY what JSON to output. We repeat
the schema in every prompt because models follow instructions better
when the expected format is right in front of them. No ambiguity, no
creativity in format — strict JSON only.

WHY LOW TEMPERATURE (0.3) FOR VOTES?
When we call these prompts, we use temperature=0.3 (set in
deliberation.py). Low temperature means more deterministic,
consistent output. For voting and reviewing, we want reliable
structured JSON — not creative exploration. Temperature is set
in the calling code, not in the prompts themselves.

WHAT ARE f-STRINGS?
Python f-strings (f"...") let you embed variables directly in strings.
Example: f"Hello {name}" becomes "Hello Abhishek" if name = "Abhishek".
The curly braces {variable} get replaced with the variable's value.

WHAT ARE TRIPLE-QUOTED STRINGS?
Python strings wrapped in triple quotes (three single or double quotes)
can span multiple lines. This is how we write long prompts without
using string concatenation or backslash line continuations.

WHAT IS THE DOUBLE-BRACE {{}} SYNTAX?
Inside f-strings, if you want a literal curly brace in the output
(like in JSON), you must double it: {{ becomes { in the output.
So f'{{"vote": "local"}}' produces the text {"vote": "local"}.
This is necessary because single braces are reserved for variables.
"""


def get_routing_vote_prompt(
    user_prompt: str,
    model_profiles_text: str,
    task_metadata_text: str,
) -> str:
    """
    Generate the system prompt for Phase 1: Routing Vote.

    Each council model receives this prompt and must vote on whether
    the user's prompt should be answered LOCALLY (free, on-device)
    or sent to the CLOUD (costs money, but higher quality).

    WHY EACH MODEL VOTES:
    Different models have different "opinions" because they have
    different training data and architectures. A coding model might
    think a code question is easy (vote local), while a general
    model might think it needs cloud's reasoning power. This
    diversity of perspective is the entire point of a council.

    Parameters:
    -----------
    user_prompt : str
        The actual text the user typed. The council needs to read
        this to judge its difficulty.

    model_profiles_text : str
        A formatted text block describing all local models and their
        strengths. Generated by format_model_profiles().

    task_metadata_text : str
        A formatted text block showing the task's metadata (type,
        complexity, importance, etc.). Generated by format_task_metadata().

    Returns:
    --------
    str
        The complete prompt text to send to a council model.
        The model should respond with a JSON object containing
        vote, reasoning, confidence, and suggested_cloud_tier.
    """
    return (
        f"You are a member of a Council of AI Models. Your job is to "
        f"vote on whether the following user prompt should be answered "
        f"LOCALLY (by one of the local models, at zero cost) or sent "
        f"to the CLOUD (to a more powerful model, which costs money).\n"
        f"\n"
        f"DECISION CRITERIA - vote LOCAL if:\n"
        f"- The task is simple, factual, or straightforward\n"
        f"- Any local model can handle it adequately\n"
        f"- The user has not indicated this is high-stakes or critical\n"
        f"- Sending to cloud would waste money for minimal quality gain\n"
        f"- The task involves code and a coding-specialized model is "
        f"available locally\n"
        f"- The task involves reasoning and a reasoning-specialized "
        f"model is available locally\n"
        f"\n"
        f"DECISION CRITERIA - vote CLOUD if:\n"
        f"- The task requires deep, nuanced reasoning beyond local "
        f"capability\n"
        f"- The task involves complex multi-step analysis that 8B "
        f"models struggle with\n"
        f"- The quality difference between local and cloud would be "
        f"significant\n"
        f"- The task is high-stakes where accuracy is critical\n"
        f"- The task requires very long, coherent output (2000+ words)\n"
        f"- The task requires knowledge that smaller models may not have\n"
        f"\n"
        f"AVAILABLE LOCAL MODELS:\n"
        f"{model_profiles_text}\n"
        f"\n"
        f"TASK CONTEXT:\n"
        f"{task_metadata_text}\n"
        f"\n"
        f"USER PROMPT:\n"
        f"{user_prompt}\n"
        f"\n"
        f"CLOUD TIERS (if you vote cloud, suggest which tier):\n"
        f"- budget: Gemini 2.5 Flash Lite (cheapest, fast, basic)\n"
        f"- standard: Gemini 2.5 Flash (good balance of cost and quality)\n"
        f"- premium: Gemini 2.5 Pro (strong reasoning, higher cost)\n"
        f"- luxury: Gemini 3.0 Pro Preview (frontier model, highest cost)\n"
        f"\n"
        f"You MUST respond with ONLY valid JSON in this exact format:\n"
        f"{{\n"
        f'    "vote": "local" or "cloud",\n'
        f'    "reasoning": "2-3 sentences explaining your vote",\n'
        f'    "confidence": 0.0 to 1.0,\n'
        f'    "suggested_cloud_tier": "budget" or "standard" or '
        f'"premium" or "luxury" or "none"\n'
        f"}}\n"
        f"\n"
        f"RULES:\n"
        f"- If you vote local, set suggested_cloud_tier to \"none\"\n"
        f"- If you vote cloud, suggest the appropriate tier\n"
        f"- Confidence 1.0 means you are absolutely certain\n"
        f"- Confidence 0.5 means you could go either way\n"
        f"- Output ONLY the JSON. No markdown. No code fences. "
        f"No explanation outside the JSON."
    )


def get_model_selection_prompt(
    user_prompt: str,
    model_profiles_text: str,
    task_metadata_text: str,
) -> str:
    """
    Generate the system prompt for Phase 2: Model Selection Vote.

    The council has already decided to answer locally. Now each model
    votes on WHICH local model is best suited to answer the prompt.

    WHY NOT JUST PICK THE BIGGEST MODEL?
    gpt-oss:20b is the most capable local model, but it takes ~14GB
    RAM and is very slow to load. For a simple question, qwen3:8b
    or deepseek-coder would answer faster with comparable quality.
    The council picks the RIGHT tool for the job, not always the
    biggest hammer.

    Parameters:
    -----------
    user_prompt : str
        The actual text the user typed.

    model_profiles_text : str
        Formatted text describing all local models and their strengths.

    task_metadata_text : str
        Formatted text showing the task's metadata.

    Returns:
    --------
    str
        The complete prompt text. The model should respond with JSON
        containing selected_model, reasoning, and confidence.
    """
    return (
        f"You are a member of a Council of AI Models. The council has "
        f"decided to answer this prompt LOCALLY. Your job is to vote "
        f"on which local model is best suited to answer it.\n"
        f"\n"
        f"Consider these factors when choosing:\n"
        f"- Task requirements: Is it coding? Reasoning? General "
        f"knowledge? Creative writing?\n"
        f"- Model strengths: Each model excels at different things\n"
        f"- Speed vs quality: Simple tasks should use faster models\n"
        f"- RAM constraints: gpt-oss:20b uses 14GB and is slow to "
        f"load. Only recommend it for tasks that truly need its "
        f"superior reasoning.\n"
        f"\n"
        f"AVAILABLE LOCAL MODELS:\n"
        f"{model_profiles_text}\n"
        f"\n"
        f"TASK CONTEXT:\n"
        f"{task_metadata_text}\n"
        f"\n"
        f"USER PROMPT:\n"
        f"{user_prompt}\n"
        f"\n"
        f"You MUST respond with ONLY valid JSON in this exact format:\n"
        f"{{\n"
        f'    "selected_model": "ollama/model-name-here",\n'
        f'    "reasoning": "2-3 sentences explaining your choice",\n'
        f'    "confidence": 0.0 to 1.0\n'
        f"}}\n"
        f"\n"
        f"The selected_model MUST be one of these exact strings:\n"
        f"- \"ollama/gpt-oss-20b\"\n"
        f"- \"ollama/llama3.1-8b\"\n"
        f"- \"ollama/deepseek-r1-8b\"\n"
        f"- \"ollama/qwen3-8b\"\n"
        f"- \"ollama/deepseek-coder-6.7b\"\n"
        f"\n"
        f"IMPORTANT:\n"
        f"- Use the exact model ID strings listed above\n"
        f"- Do not invent model names\n"
        f"- Output ONLY the JSON. No markdown. No code fences. "
        f"No explanation outside the JSON."
    )


def get_answer_review_prompt(
    user_prompt: str,
    answer: str,
    iteration: int,
    previous_feedback: str = "",
) -> str:
    """
    Generate the system prompt for Phase 3: Answer Review.

    Each council model reviews the generated answer and decides
    whether to approve or reject it, providing specific feedback.

    WHY ITERATIVE REVIEW?
    The first draft of an answer might have issues: factual errors,
    incomplete reasoning, poor structure. By having the council
    review and provide feedback, the answering model can improve.
    This is like code review but for AI responses.

    Each iteration can get stricter or more lenient:
    - Iteration 1: Thorough review, identify real issues
    - Iteration 2: Check if previous issues were addressed
    - Iteration 3: Be lenient, accept if reasonably good

    Parameters:
    -----------
    user_prompt : str
        The original question the user asked.

    answer : str
        The current answer being reviewed (may have been improved
        from previous iterations).

    iteration : int
        Which review round this is (1, 2, or 3).
        Used to adjust strictness.

    previous_feedback : str
        Aggregated feedback from previous review rounds.
        Empty string on the first iteration.

    Returns:
    --------
    str
        The complete prompt text. The model should respond with JSON
        containing approved, quality_score, feedback, and issues.
    """
    # Build the feedback section only if there is previous feedback
    feedback_section = ""
    if previous_feedback:
        feedback_section = (
            f"\nPREVIOUS FEEDBACK FROM COUNCIL (iteration "
            f"{iteration - 1}):\n"
            f"{previous_feedback}\n"
            f"\n"
            f"The answering model attempted to address this feedback. "
            f"Evaluate whether the issues have been resolved.\n"
        )

    # Adjust strictness based on iteration
    # On later iterations, be more lenient to avoid infinite loops
    if iteration >= 3:
        strictness_instruction = (
            "This is the final review round. Be LENIENT. "
            "Only reject if there are serious factual errors or "
            "the answer completely misses the question. "
            "Minor style issues should be accepted."
        )
    elif iteration == 2:
        strictness_instruction = (
            "This is the second review round. Focus on whether "
            "the previous feedback was addressed. Be fair but "
            "do not introduce new nitpicks."
        )
    else:
        strictness_instruction = (
            "This is the first review round. Be thorough but "
            "constructive. Identify genuine quality issues, not "
            "minor stylistic preferences."
        )

    return (
        f"You are a member of a Council of AI Models reviewing an "
        f"answer generated by a fellow council member.\n"
        f"\n"
        f"ORIGINAL USER PROMPT:\n"
        f"{user_prompt}\n"
        f"\n"
        f"GENERATED ANSWER (iteration {iteration}):\n"
        f"{answer}\n"
        f"{feedback_section}"
        f"\n"
        f"REVIEW CRITERIA:\n"
        f"1. ACCURACY - Is the answer factually correct?\n"
        f"2. COMPLETENESS - Does it fully address the user's question?\n"
        f"3. CLARITY - Is it well-structured and easy to understand?\n"
        f"4. RELEVANCE - Does it stay on topic without unnecessary "
        f"filler?\n"
        f"\n"
        f"STRICTNESS LEVEL:\n"
        f"{strictness_instruction}\n"
        f"\n"
        f"IMPORTANT RULES:\n"
        f"- Be constructive but honest\n"
        f"- Only reject if there are genuine quality issues\n"
        f"- Do NOT reject for minor stylistic preferences\n"
        f"- If the answer is good enough for the user's needs, "
        f"approve it\n"
        f"- Provide SPECIFIC, ACTIONABLE feedback if rejecting\n"
        f"\n"
        f"You MUST respond with ONLY valid JSON in this exact format:\n"
        f"{{\n"
        f'    "approved": true or false,\n'
        f'    "quality_score": 0.0 to 1.0,\n'
        f'    "feedback": "Specific actionable feedback, or empty '
        f'string if approved",\n'
        f'    "issues": ["issue 1", "issue 2"] or []\n'
        f"}}\n"
        f"\n"
        f"RULES:\n"
        f"- approved: true if the answer meets quality standards\n"
        f"- quality_score: 0.0 is terrible, 1.0 is perfect\n"
        f"- feedback: empty string \"\" if approved, specific text "
        f"if rejected\n"
        f"- issues: empty list [] if approved, list of strings "
        f"if rejected\n"
        f"- Output ONLY the JSON. No markdown. No code fences. "
        f"No explanation outside the JSON."
    )


def format_model_profiles(profiles: dict) -> str:
    """
    Format model profiles into a readable text block for prompts.

    This text is injected into every council prompt so that each
    model knows what the other models are capable of. Without this,
    the models would vote blindly — they'd have no idea that
    deepseek-coder is specialized for code or that gpt-oss:20b
    is the strongest but uses 14GB RAM.

    Parameters:
    -----------
    profiles : dict
        Dictionary mapping model IDs to ModelProfile objects.
        Example: {"ollama/llama3.1-8b": ModelProfile(...), ...}

    Returns:
    --------
    str
        A multi-line string where each line describes one model.

    Example output:
        - ollama/llama3.1-8b: Strong instruction following... [Speed: medium, RAM: 6.0GB]
        - ollama/deepseek-coder-6.7b: Specialized for code... [Speed: fast, RAM: 4.5GB]
    """
    lines = []
    for model_id, profile in profiles.items():
        lines.append(
            f"- {model_id}: {profile.strengths} "
            f"[Speed: {profile.speed}, RAM: {profile.ram_gb}GB]"
        )
    return "\n".join(lines)


def format_task_metadata(meta) -> str:
    """
    Format a TaskMetadata object into readable text for prompts.

    This gives the council context about the task's characteristics
    so they can make informed routing and selection decisions.

    The complexity, importance, and budget_sensitivity values are
    on a 0-1 scale where:
    - 0.0 = low (simple, unimportant, spend freely)
    - 1.0 = high (complex, critical, minimize cost)

    Parameters:
    -----------
    meta : TaskMetadata
        The task metadata object containing type, complexity,
        importance, sensitivity, etc.

    Returns:
    --------
    str
        A multi-line string describing the task's characteristics.

    Example output:
        Task Type: deep_research
        Complexity: 0.9 (0=simple, 1=very complex)
        Importance: 0.8 (0=low stakes, 1=critical)
        Data Sensitivity: low
        Budget Sensitivity: 0.5 (0=spend freely, 1=minimize cost)
    """
    return (
        f"Task Type: {meta.task_type.value}\n"
        f"Complexity: {meta.complexity} (0=simple, 1=very complex)\n"
        f"Importance: {meta.importance} (0=low stakes, 1=critical)\n"
        f"Data Sensitivity: {meta.data_sensitivity.value}\n"
        f"Budget Sensitivity: {meta.budget_sensitivity} "
        f"(0=spend freely, 1=minimize cost)"
    )
